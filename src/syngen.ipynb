{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cb794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 12:35:50 [__init__.py:243] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Prevent Python from generating .pyc files (compiled bytecode files)\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import necessary modules and configuration settings\n",
    "from prompts import *\n",
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1c15a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79d566bf89846769e3f223c824ae2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:53] ================================================================================\n",
      "[12:35:53] =                        Enrich Concepts with Synonyms                         =\n",
      "[12:35:53] ================================================================================\n",
      "[12:35:53] Reading file 'hpo.data.pkl'.\n",
      "[12:35:53] Reading file 'hpo.data.pkl' completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:47] Removing unnecessary data.\n",
      "[12:36:47] Removed 306391 entries.\n",
      "[12:36:47] Removing empty data.\n",
      "[12:36:47] Removed 0 entries.\n",
      "[12:36:47] Left with 9743 entries.\n"
     ]
    }
   ],
   "source": [
    "printHeader(f\"Enrich Concepts with Synonyms\")\n",
    "\n",
    "hpoIDs = testIDs\n",
    "\n",
    "# Load the dataset from a pickle file\n",
    "data = readPickle(inputFile)\n",
    "\n",
    "parents = [[]  for _ in range(0, len(hpoIDs))]\n",
    "children = [[]  for _ in range(0, len(hpoIDs))]\n",
    "\n",
    "with newProgress() as progress:\n",
    "\n",
    "    task = newTask(progress, len(hpoIDs), \"Get Parents and Children\")\n",
    "\n",
    "    for index in range(0, len(hpoIDs)):\n",
    "        children[index] = getChildLabels(data, hpoIDs[index])\n",
    "        parents[index] = getParentLabels(data, hpoIDs[index])\n",
    "        progress.update(task, advance=1)\n",
    "\n",
    "log(\"Removing unnecessary data.\")\n",
    "lostByFilter = len(data.index)\n",
    "data = data[data[hpoidColumn].isin(hpoIDs)]\n",
    "log(f\"Removed {lostByFilter - len(data.index)} entries.\")\n",
    "\n",
    "data[systemColumn] = [\"\"] * len(data.index)\n",
    "data[countColumn] = [1] * len(data.index)\n",
    "\n",
    "log(\"Removing empty data.\")\n",
    "data, lostByFilter = removeEmptyRows(data=data)\n",
    "log(f\"Removed {lostByFilter} entries.\")\n",
    "\n",
    "log(f\"Left with {len(data.index)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b74ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:47] Adding Definitions\n",
      "[12:36:47] Set up the LLM.\n",
      "INFO 11-28 12:36:47 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 11-28 12:36:47 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 11-28 12:36:47 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-28 12:36:59 [config.py:793] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 11-28 12:37:00 [config.py:1875] Defaulting to use mp for distributed inference\n",
      "INFO 11-28 12:37:00 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-28 12:37:03 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 11-28 12:37:03 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='google/medgemma-27b-text-it', speculative_config=None, tokenizer='google/medgemma-27b-text-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/medgemma-27b-text-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 11-28 12:37:03 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-28 12:37:03 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_0cd7c365'), local_subscribe_addr='ipc:///tmp/f38597ca-3135-4fd9-89c7-b36e4283d55c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 11-28 12:37:03 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fca4fe62c20>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:03 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_60e53edb'), local_subscribe_addr='ipc:///tmp/bdece25e-13be-47f0-bb25-41c573949c95', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 11-28 12:37:03 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fca4fe61d50>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:03 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bb104499'), local_subscribe_addr='ipc:///tmp/bba084f2-9e29-4553-bc43-71709aeae3d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 11-28 12:37:03 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fca4fe62c80>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:03 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_15275367'), local_subscribe_addr='ipc:///tmp/c609a78c-3861-449e-b596-e037e5febf4d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 11-28 12:37:03 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fca4fe62d70>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:03 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1439b562'), local_subscribe_addr='ipc:///tmp/dcf9a2fd-a5f6-4a5f-9875-03665acbd2df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:06 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:06 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:06 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "INFO 11-28 12:37:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:06 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "INFO 11-28 12:37:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m WARNING 11-28 12:37:07 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m WARNING 11-28 12:37:07 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-28 12:37:07 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-28 12:37:07 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:07 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1df8837b'), local_subscribe_addr='ipc:///tmp/6f53c501-5094-497d-8836-c4cbb28c4809', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:07 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 12:37:07 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-28 12:37:07 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-28 12:37:07 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m WARNING 11-28 12:37:07 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-28 12:37:07 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m WARNING 11-28 12:37:07 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-28 12:37:07 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:07 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:07 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:07 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "INFO 11-28 12:37:07 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:07 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:07 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 11-28 12:37:07 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:07 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:07 [backends.py:35] Using InductorAdaptor\n",
      "INFO 11-28 12:37:07 [backends.py:35] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:07 [backends.py:35] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:07 [backends.py:35] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:08 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:08 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:08 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:08 [weight_utils.py:291] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591aa4d64ed64b02917838462a0c5542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:16 [default_loader.py:280] Loading weights took 8.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:17 [default_loader.py:280] Loading weights took 8.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:17 [gpu_model_runner.py:1549] Model loading took 12.9584 GiB and 9.208835 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:17 [default_loader.py:280] Loading weights took 8.28 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:17 [gpu_model_runner.py:1549] Model loading took 12.9584 GiB and 9.492649 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:17 [gpu_model_runner.py:1549] Model loading took 12.9584 GiB and 9.815080 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:17 [default_loader.py:280] Loading weights took 8.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:18 [gpu_model_runner.py:1549] Model loading took 12.9584 GiB and 10.401326 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:34 [backends.py:459] Using cache directory: /home/pallaoro/.cache/vllm/torch_compile_cache/d74997b4fd/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:34 [backends.py:469] Dynamo bytecode transform time: 16.48 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:459] Using cache directory: /home/pallaoro/.cache/vllm/torch_compile_cache/d74997b4fd/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:469] Dynamo bytecode transform time: 16.94 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:459] Using cache directory: /home/pallaoro/.cache/vllm/torch_compile_cache/d74997b4fd/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:469] Dynamo bytecode transform time: 16.98 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:459] Using cache directory: /home/pallaoro/.cache/vllm/torch_compile_cache/d74997b4fd/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:35 [backends.py:469] Dynamo bytecode transform time: 17.05 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:49 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 14.033 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:50 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 14.103 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m INFO 11-28 12:37:50 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 14.116 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:50 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 14.077 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:37:52 [monitor.py:33] torch.compile takes 16.48 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:37:52 [monitor.py:33] torch.compile takes 16.94 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:37:52 [monitor.py:33] torch.compile takes 16.98 s in total\n",
      "INFO 11-28 12:37:52 [monitor.py:33] torch.compile takes 17.05 s in total\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:637] GPU KV cache size: 199,952 tokens\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:640] Maximum concurrency for 32,768 tokens per request: 6.10x\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:637] GPU KV cache size: 199,952 tokens\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:640] Maximum concurrency for 32,768 tokens per request: 6.10x\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:637] GPU KV cache size: 199,952 tokens\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:640] Maximum concurrency for 32,768 tokens per request: 6.10x\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:637] GPU KV cache size: 199,952 tokens\n",
      "INFO 11-28 12:37:58 [kv_cache_utils.py:640] Maximum concurrency for 32,768 tokens per request: 6.10x\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1286472)\u001b[0;0m INFO 11-28 12:38:44 [gpu_model_runner.py:1933] Graph capturing finished in 45 secs, took 1.34 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1286469)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1286471)\u001b[0;0m INFO 11-28 12:38:44 [gpu_model_runner.py:1933] Graph capturing finished in 45 secs, took 1.34 GiB\n",
      "INFO 11-28 12:38:44 [gpu_model_runner.py:1933] Graph capturing finished in 45 secs, took 1.34 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1286470)\u001b[0;0m INFO 11-28 12:38:44 [gpu_model_runner.py:1933] Graph capturing finished in 45 secs, took 1.34 GiB\n",
      "INFO 11-28 12:38:44 [core.py:167] init engine (profile, create kv cache, warmup model) took 85.99 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 11-28 12:45:57 [dump_input.py:68] Dumping input data\n",
      "ERROR 11-28 12:45:57 [dump_input.py:70] V1 LLM engine (v0.9.0.1) with config: model='google/medgemma-27b-text-it', speculative_config=None, tokenizer='google/medgemma-27b-text-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/medgemma-27b-text-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}, \n",
      "ERROR 11-28 12:45:57 [dump_input.py:78] Dumping scheduler output for model execution:\n",
      "ERROR 11-28 12:45:57 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='111', resumed_from_preemption=false, new_token_ids=[568], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='137', resumed_from_preemption=false, new_token_ids=[11258], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='138', resumed_from_preemption=false, new_token_ids=[529], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='139', resumed_from_preemption=false, new_token_ids=[15517], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='143', resumed_from_preemption=false, new_token_ids=[10317], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='149', resumed_from_preemption=false, new_token_ids=[108], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='154', resumed_from_preemption=false, new_token_ids=[120022], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='160', resumed_from_preemption=false, new_token_ids=[21346], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='169', resumed_from_preemption=false, new_token_ids=[580], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='177', resumed_from_preemption=false, new_token_ids=[140], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='178', resumed_from_preemption=false, new_token_ids=[528], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='179', resumed_from_preemption=false, new_token_ids=[779], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='182', resumed_from_preemption=false, new_token_ids=[584], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='185', resumed_from_preemption=false, new_token_ids=[25944], new_block_ids=[[]], num_computed_tokens=943), CachedRequestData(req_id='196', resumed_from_preemption=false, new_token_ids=[623], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='199', resumed_from_preemption=false, new_token_ids=[563], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='204', resumed_from_preemption=false, new_token_ids=[11825], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='207', resumed_from_preemption=false, new_token_ids=[522], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='209', resumed_from_preemption=false, new_token_ids=[236829], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='210', resumed_from_preemption=false, new_token_ids=[1018], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='211', resumed_from_preemption=false, new_token_ids=[236775], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='213', resumed_from_preemption=false, new_token_ids=[837], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='216', resumed_from_preemption=false, new_token_ids=[11825], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='224', resumed_from_preemption=false, new_token_ids=[139], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='226', resumed_from_preemption=false, new_token_ids=[496], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='227', resumed_from_preemption=false, new_token_ids=[45000], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='228', resumed_from_preemption=false, new_token_ids=[28257], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='229', resumed_from_preemption=false, new_token_ids=[236751], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='230', resumed_from_preemption=false, new_token_ids=[1018], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='236', resumed_from_preemption=false, new_token_ids=[580], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='237', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=942), CachedRequestData(req_id='238', resumed_from_preemption=false, new_token_ids=[699], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='241', resumed_from_preemption=false, new_token_ids=[17524], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='246', resumed_from_preemption=false, new_token_ids=[66515], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='251', resumed_from_preemption=false, new_token_ids=[236825], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='255', resumed_from_preemption=false, new_token_ids=[22454], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='261', resumed_from_preemption=false, new_token_ids=[2963], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='264', resumed_from_preemption=false, new_token_ids=[236778], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='267', resumed_from_preemption=false, new_token_ids=[60621], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='272', resumed_from_preemption=false, new_token_ids=[804], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='275', resumed_from_preemption=false, new_token_ids=[97749], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='283', resumed_from_preemption=false, new_token_ids=[1056], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='286', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=940), CachedRequestData(req_id='288', resumed_from_preemption=false, new_token_ids=[81047], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='289', resumed_from_preemption=false, new_token_ids=[605], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='291', resumed_from_preemption=false, new_token_ids=[1694], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='294', resumed_from_preemption=false, new_token_ids=[1982], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='295', resumed_from_preemption=false, new_token_ids=[563], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='297', resumed_from_preemption=false, new_token_ids=[90380], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='301', resumed_from_preemption=false, new_token_ids=[605], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='304', resumed_from_preemption=false, new_token_ids=[4110], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='311', resumed_from_preemption=false, new_token_ids=[3055], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='312', resumed_from_preemption=false, new_token_ids=[1651], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='313', resumed_from_preemption=false, new_token_ids=[658], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='315', resumed_from_preemption=false, new_token_ids=[630], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='317', resumed_from_preemption=false, new_token_ids=[3530], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='321', resumed_from_preemption=false, new_token_ids=[496], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='322', resumed_from_preemption=false, new_token_ids=[1174], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='323', resumed_from_preemption=false, new_token_ids=[7949], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='325', resumed_from_preemption=false, new_token_ids=[3043], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='327', resumed_from_preemption=false, new_token_ids=[2351], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='329', resumed_from_preemption=false, new_token_ids=[2395], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='334', resumed_from_preemption=false, new_token_ids=[630], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='335', resumed_from_preemption=false, new_token_ids=[3755], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='336', resumed_from_preemption=false, new_token_ids=[1049], new_block_ids=[[]], num_computed_tokens=970), CachedRequestData(req_id='343', resumed_from_preemption=false, new_token_ids=[149681], new_block_ids=[[]], num_computed_tokens=938), CachedRequestData(req_id='344', resumed_from_preemption=false, new_token_ids=[120022], new_block_ids=[[]], num_computed_tokens=786), CachedRequestData(req_id='350', resumed_from_preemption=false, new_token_ids=[618], new_block_ids=[[]], num_computed_tokens=738), CachedRequestData(req_id='352', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=731), CachedRequestData(req_id='354', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=729), CachedRequestData(req_id='355', resumed_from_preemption=false, new_token_ids=[2730], new_block_ids=[[]], num_computed_tokens=729), CachedRequestData(req_id='357', resumed_from_preemption=false, new_token_ids=[26463], new_block_ids=[[]], num_computed_tokens=714), CachedRequestData(req_id='362', resumed_from_preemption=false, new_token_ids=[531], new_block_ids=[[]], num_computed_tokens=701), CachedRequestData(req_id='363', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=701), CachedRequestData(req_id='365', resumed_from_preemption=false, new_token_ids=[16977], new_block_ids=[[]], num_computed_tokens=696), CachedRequestData(req_id='368', resumed_from_preemption=false, new_token_ids=[236789], new_block_ids=[[]], num_computed_tokens=693), CachedRequestData(req_id='369', resumed_from_preemption=false, new_token_ids=[49643], new_block_ids=[[]], num_computed_tokens=691), CachedRequestData(req_id='374', resumed_from_preemption=false, new_token_ids=[3495], new_block_ids=[[]], num_computed_tokens=678), CachedRequestData(req_id='375', resumed_from_preemption=false, new_token_ids=[531], new_block_ids=[[]], num_computed_tokens=678), CachedRequestData(req_id='376', resumed_from_preemption=false, new_token_ids=[991], new_block_ids=[[]], num_computed_tokens=677), CachedRequestData(req_id='381', resumed_from_preemption=false, new_token_ids=[9603], new_block_ids=[[]], num_computed_tokens=658), CachedRequestData(req_id='383', resumed_from_preemption=false, new_token_ids=[532], new_block_ids=[[]], num_computed_tokens=653), CachedRequestData(req_id='385', resumed_from_preemption=false, new_token_ids=[138], new_block_ids=[[]], num_computed_tokens=651), CachedRequestData(req_id='388', resumed_from_preemption=false, new_token_ids=[597], new_block_ids=[[]], num_computed_tokens=650), CachedRequestData(req_id='389', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=649), CachedRequestData(req_id='391', resumed_from_preemption=false, new_token_ids=[614], new_block_ids=[[]], num_computed_tokens=647), CachedRequestData(req_id='393', resumed_from_preemption=false, new_token_ids=[6508], new_block_ids=[[]], num_computed_tokens=645), CachedRequestData(req_id='394', resumed_from_preemption=false, new_token_ids=[107220], new_block_ids=[[]], num_computed_tokens=644), CachedRequestData(req_id='395', resumed_from_preemption=false, new_token_ids=[8514], new_block_ids=[[]], num_computed_tokens=644), CachedRequestData(req_id='397', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=642), CachedRequestData(req_id='399', resumed_from_preemption=false, new_token_ids=[138], new_block_ids=[[]], num_computed_tokens=638), CachedRequestData(req_id='400', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=637), CachedRequestData(req_id='401', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=635), CachedRequestData(req_id='403', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=627), CachedRequestData(req_id='405', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=621), CachedRequestData(req_id='406', resumed_from_preemption=false, new_token_ids=[11569], new_block_ids=[[]], num_computed_tokens=615), CachedRequestData(req_id='407', resumed_from_preemption=false, new_token_ids=[110310], new_block_ids=[[]], num_computed_tokens=614), CachedRequestData(req_id='408', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=612), CachedRequestData(req_id='409', resumed_from_preemption=false, new_token_ids=[116303], new_block_ids=[[]], num_computed_tokens=610), CachedRequestData(req_id='410', resumed_from_preemption=false, new_token_ids=[57972], new_block_ids=[[3329]], num_computed_tokens=608), CachedRequestData(req_id='412', resumed_from_preemption=false, new_token_ids=[7317], new_block_ids=[[]], num_computed_tokens=599), CachedRequestData(req_id='413', resumed_from_preemption=false, new_token_ids=[29984], new_block_ids=[[]], num_computed_tokens=598), CachedRequestData(req_id='414', resumed_from_preemption=false, new_token_ids=[116363], new_block_ids=[[]], num_computed_tokens=597), CachedRequestData(req_id='415', resumed_from_preemption=false, new_token_ids=[580], new_block_ids=[[]], num_computed_tokens=597), CachedRequestData(req_id='417', resumed_from_preemption=false, new_token_ids=[3750], new_block_ids=[[]], num_computed_tokens=594), CachedRequestData(req_id='419', resumed_from_preemption=false, new_token_ids=[8514], new_block_ids=[[]], num_computed_tokens=582), CachedRequestData(req_id='420', resumed_from_preemption=false, new_token_ids=[30672], new_block_ids=[[]], num_computed_tokens=582), CachedRequestData(req_id='421', resumed_from_preemption=false, new_token_ids=[7317], new_block_ids=[[]], num_computed_tokens=578), CachedRequestData(req_id='422', resumed_from_preemption=false, new_token_ids=[11569], new_block_ids=[[7278]], num_computed_tokens=576), CachedRequestData(req_id='423', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=575), CachedRequestData(req_id='424', resumed_from_preemption=false, new_token_ids=[8427], new_block_ids=[[]], num_computed_tokens=573), CachedRequestData(req_id='425', resumed_from_preemption=false, new_token_ids=[840], new_block_ids=[[]], num_computed_tokens=569), CachedRequestData(req_id='426', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=569), CachedRequestData(req_id='427', resumed_from_preemption=false, new_token_ids=[71882], new_block_ids=[[]], num_computed_tokens=567), CachedRequestData(req_id='428', resumed_from_preemption=false, new_token_ids=[58944], new_block_ids=[[]], num_computed_tokens=563), CachedRequestData(req_id='429', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=562), CachedRequestData(req_id='430', resumed_from_preemption=false, new_token_ids=[919], new_block_ids=[[]], num_computed_tokens=558), CachedRequestData(req_id='431', resumed_from_preemption=false, new_token_ids=[651], new_block_ids=[[]], num_computed_tokens=557), CachedRequestData(req_id='432', resumed_from_preemption=false, new_token_ids=[531], new_block_ids=[[]], num_computed_tokens=554), CachedRequestData(req_id='433', resumed_from_preemption=false, new_token_ids=[1061], new_block_ids=[[]], num_computed_tokens=548), CachedRequestData(req_id='434', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=546), CachedRequestData(req_id='435', resumed_from_preemption=false, new_token_ids=[8398], new_block_ids=[[]], num_computed_tokens=545), CachedRequestData(req_id='436', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=534), CachedRequestData(req_id='437', resumed_from_preemption=false, new_token_ids=[116363], new_block_ids=[[]], num_computed_tokens=530), CachedRequestData(req_id='441', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=562), CachedRequestData(req_id='442', resumed_from_preemption=false, new_token_ids=[168571], new_block_ids=[[]], num_computed_tokens=559), CachedRequestData(req_id='443', resumed_from_preemption=false, new_token_ids=[108], new_block_ids=[[]], num_computed_tokens=558), CachedRequestData(req_id='444', resumed_from_preemption=false, new_token_ids=[127564], new_block_ids=[[]], num_computed_tokens=557), CachedRequestData(req_id='445', resumed_from_preemption=false, new_token_ids=[1492], new_block_ids=[[]], num_computed_tokens=556), CachedRequestData(req_id='447', resumed_from_preemption=false, new_token_ids=[2098], new_block_ids=[[]], num_computed_tokens=554), CachedRequestData(req_id='448', resumed_from_preemption=false, new_token_ids=[528], new_block_ids=[[]], num_computed_tokens=552), CachedRequestData(req_id='449', resumed_from_preemption=false, new_token_ids=[1174], new_block_ids=[[]], num_computed_tokens=552), CachedRequestData(req_id='450', resumed_from_preemption=false, new_token_ids=[3187], new_block_ids=[[]], num_computed_tokens=548), CachedRequestData(req_id='451', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[7021]], num_computed_tokens=544), CachedRequestData(req_id='452', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=541), CachedRequestData(req_id='455', resumed_from_preemption=false, new_token_ids=[107471], new_block_ids=[[]], num_computed_tokens=539), CachedRequestData(req_id='456', resumed_from_preemption=false, new_token_ids=[25864], new_block_ids=[[]], num_computed_tokens=538), CachedRequestData(req_id='457', resumed_from_preemption=false, new_token_ids=[3439], new_block_ids=[[]], num_computed_tokens=536), CachedRequestData(req_id='458', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=536), CachedRequestData(req_id='459', resumed_from_preemption=false, new_token_ids=[236829], new_block_ids=[[]], num_computed_tokens=533), CachedRequestData(req_id='460', resumed_from_preemption=false, new_token_ids=[13855], new_block_ids=[[6761]], num_computed_tokens=528), CachedRequestData(req_id='461', resumed_from_preemption=false, new_token_ids=[840], new_block_ids=[[]], num_computed_tokens=525), CachedRequestData(req_id='462', resumed_from_preemption=false, new_token_ids=[5511], new_block_ids=[[]], num_computed_tokens=524), CachedRequestData(req_id='463', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=523), CachedRequestData(req_id='464', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=514), CachedRequestData(req_id='465', resumed_from_preemption=false, new_token_ids=[11616], new_block_ids=[[]], num_computed_tokens=513), CachedRequestData(req_id='466', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=507), CachedRequestData(req_id='467', resumed_from_preemption=false, new_token_ids=[2540], new_block_ids=[[]], num_computed_tokens=501), CachedRequestData(req_id='468', resumed_from_preemption=false, new_token_ids=[614], new_block_ids=[[]], num_computed_tokens=500), CachedRequestData(req_id='469', resumed_from_preemption=false, new_token_ids=[496], new_block_ids=[[]], num_computed_tokens=497), CachedRequestData(req_id='470', resumed_from_preemption=false, new_token_ids=[127564], new_block_ids=[[]], num_computed_tokens=493), CachedRequestData(req_id='471', resumed_from_preemption=false, new_token_ids=[13918], new_block_ids=[[]], num_computed_tokens=491), CachedRequestData(req_id='472', resumed_from_preemption=false, new_token_ids=[14361], new_block_ids=[[]], num_computed_tokens=483), CachedRequestData(req_id='473', resumed_from_preemption=false, new_token_ids=[1638], new_block_ids=[[]], num_computed_tokens=483), CachedRequestData(req_id='474', resumed_from_preemption=false, new_token_ids=[47875], new_block_ids=[[]], num_computed_tokens=478), CachedRequestData(req_id='475', resumed_from_preemption=false, new_token_ids=[1056], new_block_ids=[[]], num_computed_tokens=477), CachedRequestData(req_id='476', resumed_from_preemption=false, new_token_ids=[34331], new_block_ids=[[]], num_computed_tokens=475), CachedRequestData(req_id='477', resumed_from_preemption=false, new_token_ids=[139], new_block_ids=[[6504]], num_computed_tokens=464), CachedRequestData(req_id='478', resumed_from_preemption=false, new_token_ids=[10916], new_block_ids=[[]], num_computed_tokens=463), CachedRequestData(req_id='479', resumed_from_preemption=false, new_token_ids=[568], new_block_ids=[[]], num_computed_tokens=459), CachedRequestData(req_id='480', resumed_from_preemption=false, new_token_ids=[13959], new_block_ids=[[]], num_computed_tokens=458), CachedRequestData(req_id='481', resumed_from_preemption=false, new_token_ids=[529], new_block_ids=[[]], num_computed_tokens=456), CachedRequestData(req_id='482', resumed_from_preemption=false, new_token_ids=[34331], new_block_ids=[[]], num_computed_tokens=455), CachedRequestData(req_id='483', resumed_from_preemption=false, new_token_ids=[5978], new_block_ids=[[]], num_computed_tokens=452), CachedRequestData(req_id='484', resumed_from_preemption=false, new_token_ids=[12502], new_block_ids=[[]], num_computed_tokens=450), CachedRequestData(req_id='485', resumed_from_preemption=false, new_token_ids=[3649], new_block_ids=[[]], num_computed_tokens=449), CachedRequestData(req_id='486', resumed_from_preemption=false, new_token_ids=[5978], new_block_ids=[[]], num_computed_tokens=436), CachedRequestData(req_id='487', resumed_from_preemption=false, new_token_ids=[17230], new_block_ids=[[6247]], num_computed_tokens=432), CachedRequestData(req_id='488', resumed_from_preemption=false, new_token_ids=[4194], new_block_ids=[[]], num_computed_tokens=391), CachedRequestData(req_id='489', resumed_from_preemption=false, new_token_ids=[118729], new_block_ids=[[]], num_computed_tokens=390), CachedRequestData(req_id='490', resumed_from_preemption=false, new_token_ids=[21191], new_block_ids=[[]], num_computed_tokens=389), CachedRequestData(req_id='491', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=386), CachedRequestData(req_id='492', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=385), CachedRequestData(req_id='493', resumed_from_preemption=false, new_token_ids=[5978], new_block_ids=[[]], num_computed_tokens=383), CachedRequestData(req_id='494', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=383), CachedRequestData(req_id='495', resumed_from_preemption=false, new_token_ids=[1174], new_block_ids=[[]], num_computed_tokens=383), CachedRequestData(req_id='496', resumed_from_preemption=false, new_token_ids=[4330], new_block_ids=[[]], num_computed_tokens=382), CachedRequestData(req_id='497', resumed_from_preemption=false, new_token_ids=[4453], new_block_ids=[[]], num_computed_tokens=380), CachedRequestData(req_id='498', resumed_from_preemption=false, new_token_ids=[3999], new_block_ids=[[]], num_computed_tokens=379), CachedRequestData(req_id='499', resumed_from_preemption=false, new_token_ids=[22454], new_block_ids=[[]], num_computed_tokens=378), CachedRequestData(req_id='500', resumed_from_preemption=false, new_token_ids=[41080], new_block_ids=[[]], num_computed_tokens=375), CachedRequestData(req_id='501', resumed_from_preemption=false, new_token_ids=[138], new_block_ids=[[]], num_computed_tokens=374), CachedRequestData(req_id='502', resumed_from_preemption=false, new_token_ids=[53121], new_block_ids=[[]], num_computed_tokens=369), CachedRequestData(req_id='503', resumed_from_preemption=false, new_token_ids=[120022], new_block_ids=[[]], num_computed_tokens=367), CachedRequestData(req_id='504', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=365), CachedRequestData(req_id='505', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=365), CachedRequestData(req_id='506', resumed_from_preemption=false, new_token_ids=[28257], new_block_ids=[[]], num_computed_tokens=361), CachedRequestData(req_id='507', resumed_from_preemption=false, new_token_ids=[699], new_block_ids=[[]], num_computed_tokens=355), CachedRequestData(req_id='508', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=351), CachedRequestData(req_id='509', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=350), CachedRequestData(req_id='510', resumed_from_preemption=false, new_token_ids=[32427], new_block_ids=[[]], num_computed_tokens=348), CachedRequestData(req_id='511', resumed_from_preemption=false, new_token_ids=[1941], new_block_ids=[[]], num_computed_tokens=348), CachedRequestData(req_id='512', resumed_from_preemption=false, new_token_ids=[3392], new_block_ids=[[]], num_computed_tokens=345), CachedRequestData(req_id='513', resumed_from_preemption=false, new_token_ids=[897], new_block_ids=[[]], num_computed_tokens=343), CachedRequestData(req_id='514', resumed_from_preemption=false, new_token_ids=[1174], new_block_ids=[[]], num_computed_tokens=339), CachedRequestData(req_id='515', resumed_from_preemption=false, new_token_ids=[731], new_block_ids=[[]], num_computed_tokens=337), CachedRequestData(req_id='516', resumed_from_preemption=false, new_token_ids=[706], new_block_ids=[[]], num_computed_tokens=333), CachedRequestData(req_id='517', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=332), CachedRequestData(req_id='518', resumed_from_preemption=false, new_token_ids=[236812], new_block_ids=[[]], num_computed_tokens=327), CachedRequestData(req_id='519', resumed_from_preemption=false, new_token_ids=[53121], new_block_ids=[[]], num_computed_tokens=326), CachedRequestData(req_id='520', resumed_from_preemption=false, new_token_ids=[496], new_block_ids=[[]], num_computed_tokens=326), CachedRequestData(req_id='521', resumed_from_preemption=false, new_token_ids=[53121], new_block_ids=[[]], num_computed_tokens=324), CachedRequestData(req_id='522', resumed_from_preemption=false, new_token_ids=[236829], new_block_ids=[[]], num_computed_tokens=323), CachedRequestData(req_id='523', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=322), CachedRequestData(req_id='524', resumed_from_preemption=false, new_token_ids=[43248], new_block_ids=[[]], num_computed_tokens=321), CachedRequestData(req_id='525', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=317), CachedRequestData(req_id='526', resumed_from_preemption=false, new_token_ids=[3877], new_block_ids=[[]], num_computed_tokens=316), CachedRequestData(req_id='527', resumed_from_preemption=false, new_token_ids=[3877], new_block_ids=[[]], num_computed_tokens=316), CachedRequestData(req_id='528', resumed_from_preemption=false, new_token_ids=[138], new_block_ids=[[]], num_computed_tokens=314), CachedRequestData(req_id='529', resumed_from_preemption=false, new_token_ids=[1941], new_block_ids=[[]], num_computed_tokens=312), CachedRequestData(req_id='530', resumed_from_preemption=false, new_token_ids=[3392], new_block_ids=[[]], num_computed_tokens=311), CachedRequestData(req_id='531', resumed_from_preemption=false, new_token_ids=[139], new_block_ids=[[]], num_computed_tokens=311), CachedRequestData(req_id='532', resumed_from_preemption=false, new_token_ids=[54945], new_block_ids=[[]], num_computed_tokens=308), CachedRequestData(req_id='533', resumed_from_preemption=false, new_token_ids=[1941], new_block_ids=[[]], num_computed_tokens=307), CachedRequestData(req_id='534', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=301), CachedRequestData(req_id='535', resumed_from_preemption=false, new_token_ids=[45882], new_block_ids=[[]], num_computed_tokens=299), CachedRequestData(req_id='536', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=298), CachedRequestData(req_id='537', resumed_from_preemption=false, new_token_ids=[1595], new_block_ids=[[]], num_computed_tokens=296), CachedRequestData(req_id='538', resumed_from_preemption=false, new_token_ids=[6868], new_block_ids=[[]], num_computed_tokens=308), CachedRequestData(req_id='539', resumed_from_preemption=false, new_token_ids=[236811], new_block_ids=[[]], num_computed_tokens=306), CachedRequestData(req_id='540', resumed_from_preemption=false, new_token_ids=[982], new_block_ids=[[]], num_computed_tokens=305), CachedRequestData(req_id='541', resumed_from_preemption=false, new_token_ids=[236761], new_block_ids=[[]], num_computed_tokens=305), CachedRequestData(req_id='542', resumed_from_preemption=false, new_token_ids=[14963], new_block_ids=[[]], num_computed_tokens=299), CachedRequestData(req_id='543', resumed_from_preemption=false, new_token_ids=[13629], new_block_ids=[[]], num_computed_tokens=299), CachedRequestData(req_id='544', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=292), CachedRequestData(req_id='545', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=291), CachedRequestData(req_id='546', resumed_from_preemption=false, new_token_ids=[138], new_block_ids=[[]], num_computed_tokens=290), CachedRequestData(req_id='547', resumed_from_preemption=false, new_token_ids=[4926], new_block_ids=[[]], num_computed_tokens=289), CachedRequestData(req_id='548', resumed_from_preemption=false, new_token_ids=[6796], new_block_ids=[[5990]], num_computed_tokens=288), CachedRequestData(req_id='549', resumed_from_preemption=false, new_token_ids=[9844], new_block_ids=[[]], num_computed_tokens=285), CachedRequestData(req_id='550', resumed_from_preemption=false, new_token_ids=[1547], new_block_ids=[[]], num_computed_tokens=283), CachedRequestData(req_id='551', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=281), CachedRequestData(req_id='552', resumed_from_preemption=false, new_token_ids=[4453], new_block_ids=[[]], num_computed_tokens=278), CachedRequestData(req_id='553', resumed_from_preemption=false, new_token_ids=[618], new_block_ids=[[]], num_computed_tokens=273), CachedRequestData(req_id='554', resumed_from_preemption=false, new_token_ids=[529], new_block_ids=[[]], num_computed_tokens=270), CachedRequestData(req_id='555', resumed_from_preemption=false, new_token_ids=[563], new_block_ids=[[]], num_computed_tokens=261), CachedRequestData(req_id='556', resumed_from_preemption=false, new_token_ids=[23906], new_block_ids=[[]], num_computed_tokens=261), CachedRequestData(req_id='557', resumed_from_preemption=false, new_token_ids=[1908], new_block_ids=[[]], num_computed_tokens=251), CachedRequestData(req_id='558', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=250), CachedRequestData(req_id='559', resumed_from_preemption=false, new_token_ids=[837], new_block_ids=[[]], num_computed_tokens=250), CachedRequestData(req_id='560', resumed_from_preemption=false, new_token_ids=[236775], new_block_ids=[[]], num_computed_tokens=249), CachedRequestData(req_id='561', resumed_from_preemption=false, new_token_ids=[4453], new_block_ids=[[]], num_computed_tokens=239), CachedRequestData(req_id='562', resumed_from_preemption=false, new_token_ids=[31983], new_block_ids=[[]], num_computed_tokens=237), CachedRequestData(req_id='563', resumed_from_preemption=false, new_token_ids=[506], new_block_ids=[[]], num_computed_tokens=237), CachedRequestData(req_id='564', resumed_from_preemption=false, new_token_ids=[108], new_block_ids=[[]], num_computed_tokens=233), CachedRequestData(req_id='565', resumed_from_preemption=false, new_token_ids=[42233], new_block_ids=[[]], num_computed_tokens=233), CachedRequestData(req_id='566', resumed_from_preemption=false, new_token_ids=[1174], new_block_ids=[[]], num_computed_tokens=233), CachedRequestData(req_id='567', resumed_from_preemption=false, new_token_ids=[12853], new_block_ids=[[]], num_computed_tokens=232), CachedRequestData(req_id='568', resumed_from_preemption=false, new_token_ids=[14963], new_block_ids=[[]], num_computed_tokens=227), CachedRequestData(req_id='569', resumed_from_preemption=false, new_token_ids=[236811], new_block_ids=[[]], num_computed_tokens=215), CachedRequestData(req_id='570', resumed_from_preemption=false, new_token_ids=[779], new_block_ids=[[]], num_computed_tokens=214), CachedRequestData(req_id='571', resumed_from_preemption=false, new_token_ids=[4885], new_block_ids=[[]], num_computed_tokens=209), CachedRequestData(req_id='572', resumed_from_preemption=false, new_token_ids=[14963], new_block_ids=[[]], num_computed_tokens=204), CachedRequestData(req_id='573', resumed_from_preemption=false, new_token_ids=[529], new_block_ids=[[]], num_computed_tokens=198), CachedRequestData(req_id='574', resumed_from_preemption=false, new_token_ids=[236764], new_block_ids=[[]], num_computed_tokens=195), CachedRequestData(req_id='575', resumed_from_preemption=false, new_token_ids=[107], new_block_ids=[[]], num_computed_tokens=193)], num_scheduled_tokens={488: 1, 139: 1, 357: 1, 483: 1, 363: 1, 558: 1, 403: 1, 199: 1, 534: 1, 408: 1, 227: 1, 521: 1, 461: 1, 419: 1, 368: 1, 321: 1, 565: 1, 352: 1, 207: 1, 226: 1, 462: 1, 365: 1, 476: 1, 449: 1, 415: 1, 427: 1, 533: 1, 531: 1, 138: 1, 507: 1, 527: 1, 213: 1, 493: 1, 297: 1, 286: 1, 555: 1, 459: 1, 261: 1, 538: 1, 216: 1, 336: 1, 399: 1, 529: 1, 401: 1, 376: 1, 409: 1, 561: 1, 291: 1, 544: 1, 564: 1, 329: 1, 301: 1, 149: 1, 466: 1, 344: 1, 246: 1, 566: 1, 288: 1, 251: 1, 381: 1, 500: 1, 385: 1, 487: 1, 350: 1, 557: 1, 388: 1, 455: 1, 541: 1, 241: 1, 445: 1, 224: 1, 572: 1, 442: 1, 491: 1, 523: 1, 511: 1, 407: 1, 547: 1, 160: 1, 111: 1, 478: 1, 503: 1, 433: 1, 486: 1, 545: 1, 209: 1, 513: 1, 560: 1, 467: 1, 490: 1, 542: 1, 177: 1, 522: 1, 185: 1, 559: 1, 468: 1, 238: 1, 312: 1, 355: 1, 537: 1, 543: 1, 550: 1, 391: 1, 508: 1, 137: 1, 335: 1, 570: 1, 436: 1, 528: 1, 515: 1, 322: 1, 460: 1, 443: 1, 210: 1, 211: 1, 530: 1, 481: 1, 228: 1, 435: 1, 441: 1, 317: 1, 204: 1, 465: 1, 497: 1, 229: 1, 431: 1, 448: 1, 394: 1, 458: 1, 475: 1, 552: 1, 479: 1, 354: 1, 512: 1, 571: 1, 323: 1, 334: 1, 423: 1, 480: 1, 536: 1, 397: 1, 413: 1, 526: 1, 375: 1, 325: 1, 369: 1, 255: 1, 452: 1, 477: 1, 562: 1, 517: 1, 169: 1, 485: 1, 563: 1, 514: 1, 474: 1, 264: 1, 524: 1, 532: 1, 553: 1, 470: 1, 472: 1, 426: 1, 400: 1, 143: 1, 556: 1, 574: 1, 498: 1, 539: 1, 473: 1, 267: 1, 484: 1, 495: 1, 516: 1, 518: 1, 554: 1, 178: 1, 395: 1, 425: 1, 434: 1, 417: 1, 182: 1, 482: 1, 237: 1, 437: 1, 289: 1, 499: 1, 549: 1, 573: 1, 494: 1, 519: 1, 567: 1, 471: 1, 389: 1, 362: 1, 456: 1, 414: 1, 327: 1, 311: 1, 315: 1, 520: 1, 535: 1, 313: 1, 343: 1, 412: 1, 489: 1, 295: 1, 374: 1, 420: 1, 504: 1, 548: 1, 464: 1, 406: 1, 196: 1, 450: 1, 393: 1, 294: 1, 428: 1, 569: 1, 410: 1, 451: 1, 568: 1, 546: 1, 275: 1, 272: 1, 230: 1, 430: 1, 457: 1, 492: 1, 509: 1, 510: 1, 463: 1, 540: 1, 551: 1, 469: 1, 496: 1, 447: 1, 432: 1, 506: 1, 501: 1, 502: 1, 444: 1, 405: 1, 421: 1, 505: 1, 525: 1, 575: 1, 154: 1, 283: 1, 304: 1, 236: 1, 429: 1, 179: 1, 422: 1, 424: 1, 383: 1}, total_num_scheduled_tokens=256, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[3], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n"
     ]
    }
   ],
   "source": [
    "log(\"Adding Definitions\")\n",
    "\n",
    "log(\"Set up the LLM.\")\n",
    "model = Model(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582aa876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:46] Found 412/434 English definitions.\n",
      "[12:38:46] 22 system instructions added to the model.\n",
      "[12:38:46] 22 prompts added to the model. Start generating responses.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe79beff7ae4cfc878319d4b4f51595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee82daa223a437fb586080f684474e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:39:40] 22 prompts added to the model. Start generating responses.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bf855475ad4e91a44254b6f30c9dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c14042c6ef1483dacfa22e22a074b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:02] 22 prompts added to the model. Start generating responses.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ced24a253b4b4688fa35066f36fe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378179c29c64442f8f38d6acee6df2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:52] 22 prompts added to the model. Start generating responses.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba64bbcc45f4c24b3681154000e2184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21b2075195844c2a72006bc792c0e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163079dc504a4128bfc57f692c8d74f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:57] ================================================================================\n",
      "[12:41:57] =                         Definition adding completed                          =\n",
      "[12:41:57] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "definitions = data.loc[(data[languageColumn] == \"en\") & \n",
    "            (data[classColumn] == definitionClass), hpoidColumn].tolist()\n",
    "\n",
    "sourceDefinitionCount = len(data.loc[\n",
    "    (data[languageColumn] == \"en\") & \n",
    "    (data[classColumn] == definitionClass), hpoidColumn].tolist())\n",
    "\n",
    "if (sourceDefinitionCount > 0):\n",
    "    log(f\"Found {sourceDefinitionCount}/{len(hpoIDs)} English \" \\\n",
    "        \"definitions.\")\n",
    "\n",
    "    noDefinitions = hpoIDs.copy()\n",
    "\n",
    "    for hpoID in definitions:\n",
    "        noDefinitions.remove(hpoID)\n",
    "\n",
    "    if (len(noDefinitions) > 0):\n",
    "\n",
    "        instructions = [getPreTaskSystem()] * len(noDefinitions)\n",
    "        log(f\"{model.addPrompt(systemRole, instructions)} system instructions \" \\\n",
    "            \"added to the model.\")\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        for hpoID in noDefinitions:\n",
    "            messages.append(getPreTaskPart1(\"\".join(getElements(data, hpoID, \"en\",labelClass))))\n",
    "\n",
    "        log(f\"{model.addPrompt(userRole, messages)} prompts added to \" \\\n",
    "            \"the model. Start generating responses.\")\n",
    "        model.generate()\n",
    "\n",
    "        messages = []\n",
    "        for hpoID in noDefinitions:\n",
    "            messages.append(getPreTaskPart2(parents[hpoIDs == hpoID]))\n",
    "\n",
    "        c = model.addPrompt(userRole, messages)\n",
    "        log(f\"{c} prompts added to the model. Start generating responses.\")\n",
    "        model.generate()\n",
    "\n",
    "        messages = []\n",
    "        for hpoID in noDefinitions:\n",
    "            messages.append(getPreTaskPart3(children[hpoIDs == hpoID]))\n",
    "\n",
    "        c = model.addPrompt(userRole, messages)\n",
    "        log(f\"{c} prompts added to the model. Start generating responses.\")\n",
    "        model.generate()\n",
    "\n",
    "        log(f\"{model.addPrompt(userRole, [getPreTaskPart4()])} \" \\\n",
    "            \"prompts added to the model. Start generating responses.\")\n",
    "        model.generate()\n",
    "\n",
    "        messageHistories = model.getMessageHistories()\n",
    "\n",
    "        definitionTexts = []\n",
    "\n",
    "        with newProgress() as progress:\n",
    "            task = newTask(progress, len(messageHistories), \"Processing gen. Text\")\n",
    "\n",
    "            for messageHistory in messageHistories:\n",
    "                definitionTexts.append(messageHistory[-1][messageTextElement].\n",
    "                    replace(\"\\n\", \"\").strip())\n",
    "                progress.update(task, advance=1)\n",
    "\n",
    "        formattedDefinition = pd.DataFrame({\n",
    "            languageColumn  : [\"en\"] * len(definitionTexts),\n",
    "            contentColumn   : definitionTexts,\n",
    "            classColumn     : [enrichedSourceDefinitionClass] * len(definitionTexts),\n",
    "            hpoidColumn     : noDefinitions#,\n",
    "            #sourceElemement : [[-1]] * len(definitions)\n",
    "        })\n",
    "\n",
    "        data = pd.concat([data, formattedDefinition])\n",
    "\n",
    "        # Reset index after cleaning\n",
    "        data = data.reset_index(drop=True)\n",
    "\n",
    "        model.logPrompts()\n",
    "\n",
    "printHeader(\"Definition adding completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6460d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdefdf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fe1d299e4e41799fd2f5f274936487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:57] 21700 system instructions added to the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:51] 21700 prompts added to the model. Start generating responses.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1efc5eb642941d8b02c5b0c436765fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/21700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf3c79874b44bd082494f9c5bfaa285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/21700 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(task, advance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39maddPrompt(userRole,\u001b[38;5;250m \u001b[39mmessages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompts added to \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model. Start generating responses.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39maddPrompt(userRole,\u001b[38;5;250m \u001b[39m[getTaskPart2()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompts \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded to the model. Start generating responses.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mgenerate()\n",
      "File \u001b[0;32m~/syngen/src/model.py:139\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, logging)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, logging : \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerateGemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerateLlama(logging)\n",
      "File \u001b[0;32m~/syngen/src/model.py:102\u001b[0m, in \u001b[0;36mModel.generateGemma\u001b[0;34m(self, logging)\u001b[0m\n\u001b[1;32m     99\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Generate response  \u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m generatedText \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m generatedText:\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/site-packages/vllm/utils.py:1218\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1215\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m         )\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/site-packages/vllm/entrypoints/llm.py:481\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    469\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    472\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    473\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority,\n\u001b[1;32m    479\u001b[0m )\n\u001b[0;32m--> 481\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1476\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1474\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1476\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:231\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m iteration_stats \u001b[38;5;241m=\u001b[39m IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:645\u001b[0m, in \u001b[0;36mSyncMPClient.get_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EngineCoreOutputs:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/user/install/envs/gpu/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "instructions = [getSystem()] * len(hpoIDs) * generateTimes\n",
    "log(f\"{model.addPrompt(systemRole, instructions)} system instructions \" \\\n",
    "    \"added to the model.\")\n",
    "\n",
    "# Add first instruction.\n",
    "messages = []\n",
    "\n",
    "with newProgress() as progress:\n",
    "\n",
    "    task = newTask(progress, len(hpoIDs) * generateTimes, \"Set System Prompt\")\n",
    "\n",
    "    for hpoID in hpoIDs:\n",
    "        for _ in range(0, generateTimes):\n",
    "            messages.append(getTaskPart1(\n",
    "                \"\".join(getElements(data, hpoID, [labelClass], \"en\")),\n",
    "                \"\".join(getElements(data, hpoID, [definitionClass, enrichedSourceDefinitionClass], \"en\"))\n",
    "            ))\n",
    "            progress.update(task, advance=1)\n",
    "\n",
    "log(f\"{model.addPrompt(userRole, messages)} prompts added to \" \\\n",
    "    \"the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "log(f\"{model.addPrompt(userRole, [getTaskPart2()])} prompts \" \\\n",
    "    \"added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "log(f\"{model.addPrompt(userRole, [getTaskPart3()])} prompts \" \\\n",
    "    \"added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "log(f\"{model.addPrompt(userRole, [getTaskPart4()])} prompts \" \\\n",
    "    \"added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "messages = []\n",
    "for index in range(0, len(hpoIDs)):\n",
    "    for _ in range(0, generateTimes):\n",
    "        messages.append(getTaskPart5(parents[index]))\n",
    "\n",
    "c = model.addPrompt(userRole, messages)\n",
    "log(f\"{c} prompts added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "messages = []\n",
    "for index in range(0, len(hpoIDs)):\n",
    "    for _ in range(0, generateTimes):\n",
    "        messages.append(getTaskPart6(children[index]))\n",
    "\n",
    "c = model.addPrompt(userRole, messages)\n",
    "log(f\"{c} prompts added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "log(f\"{model.addPrompt(userRole, [getTaskPart7()])} prompts \" \\\n",
    "    \"added to the model. Start generating responses.\")\n",
    "model.generate()\n",
    "\n",
    "model.logPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473c5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acacbfd22e8c4be2ada044242ea1db59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messagesHistories = model.getMessageHistories()\n",
    "\n",
    "synonymLists = [[]  for _ in range(0, len(messagesHistories))]\n",
    "incorrectFormats = []\n",
    "\n",
    "# Process each concept ID to enrich synonyms\n",
    "with newProgress() as progress:\n",
    "    task = newTask(progress, len(messagesHistories), \"Processing Synonyms\")\n",
    "    \n",
    "    for index, messagesHistory in enumerate(messagesHistories):\n",
    "            \n",
    "        answer = str(messagesHistory[-1][messageTextElement]).strip()\n",
    "        answer = replaceQuotes(answer.replace(\"\\n\", \"\").replace(\".\", \"\"))\n",
    "\n",
    "        tmp = list(set(formatting(answer)))\n",
    "        synonymLists[index] = [str(t).lower() for t in tmp]\n",
    "        if synonymLists[index] is not None and \"\" in synonymLists[index]:\n",
    "            synonymLists[index].remove(\"\")\n",
    "        \n",
    "        if len(synonymLists[index]) == 0:\n",
    "            if len(answer) > 0:\n",
    "                incorrectFormats.append(hpoIDs[index])\n",
    "\n",
    "        progress.update(task, advance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for index, l in enumerate(synonymLists):\n",
    "    result.append(pd.DataFrame({\n",
    "        hpoidColumn : [hpoIDs[math.floor(index / generateTimes)]] * len(l),\n",
    "        contentColumn : l,\n",
    "        classColumn : [enrichedSourceExactSynonymClass] * len(l),\n",
    "        languageColumn : [\"en\"] * len(l),\n",
    "        #sourceElemement : [-1 for _ in range(0, len(l))],\n",
    "        systemColumn : [model_id] * len(l),\n",
    "        \"round\" : [(index % generateTimes) + 1] * len(l)\n",
    "        #elementIDColumn : [getNextElementID() for _ in range(0, len(l))]\n",
    "    }))\n",
    "\n",
    "generated = pd.concat(result)\n",
    "generated = generated.drop_duplicates(ignore_index=False).reset_index(drop=True)\n",
    "\n",
    "gold = data[((data[classColumn] == labelClass) | (data[classColumn] == exactSynonymClass)) & (data[languageColumn] == \"en\")].drop(['source'], axis=1)\n",
    "gold = gold.drop_duplicates(ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "for index in range(0, len(gold.index)):\n",
    "    gold.loc[index, contentColumn] = str(gold.loc[index, contentColumn]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")\n",
    "model = BertModel.from_pretrained(\"google-bert/bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4896c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1749ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data : list) -> list:\n",
    "    ret = [[] for _ in range(0, len(data))]\n",
    "\n",
    "    for index, l in enumerate(data):\n",
    "        ret[index] = get_bert_embedding(l)\n",
    "\n",
    "    return torch.stack(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsGenerated = embed(generated[contentColumn].tolist())\n",
    "embeddingsGold = embed(gold[contentColumn].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.to_csv(outputFileGenerated)\n",
    "pd.DataFrame(embeddingsGenerated.numpy()).to_csv(outputFileGeneratedEmbeddings, index = False)\n",
    "gold.to_csv(outputFileGold)\n",
    "pd.DataFrame(embeddingsGold.numpy()).to_csv(outputFileGoldEmbeddings, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
